{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing is a programming concept where multiple processes run independently in their own memory space, each with its own Python interpreter and resources. This is in contrast to multithreading, where multiple threads share the same memory space. Multiprocessing is often used to achieve parallelism, allowing multiple tasks to be executed simultaneously and improving overall program performance. In Python, the `multiprocessing` module provides a way to create and manage processes.\n",
    "\n",
    "Here's a detailed explanation of multiprocessing with examples:\n",
    "\n",
    "### 1. Creating Processes:\n",
    "\n",
    "To create a process in Python, you typically define a function that represents the task you want the process to execute. Then, you create a `Process` object and start it.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "def print_numbers():\n",
    "    for i in range(5):\n",
    "        time.sleep(1)\n",
    "        print(f\"Process: {i}\")\n",
    "\n",
    "# Create a process\n",
    "process = Process(target=print_numbers)\n",
    "\n",
    "# Start the process\n",
    "process.start()\n",
    "\n",
    "# Wait for the process to finish\n",
    "process.join()\n",
    "\n",
    "print(\"Main process exiting.\")\n",
    "```\n",
    "\n",
    "In this example, `print_numbers` is a simple function that prints numbers with a delay. The `Process` class is used to create a new process, and the `start` method is called to begin its execution. The `join` method is then used to wait for the process to complete before the main process exits.\n",
    "\n",
    "### 2. Sharing Data between Processes:\n",
    "\n",
    "Processes in Python have their own memory space, but it's possible to share data between them using objects like `Value` and `Array` from the `multiprocessing` module.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process, Value\n",
    "import time\n",
    "\n",
    "def increment_counter(counter):\n",
    "    for _ in range(5):\n",
    "        time.sleep(1)\n",
    "        with counter.get_lock():\n",
    "            counter.value += 1\n",
    "        print(f\"Counter: {counter.value}\")\n",
    "\n",
    "# Create a shared counter\n",
    "counter = Value('i', 0)\n",
    "\n",
    "# Create a process that increments the counter\n",
    "process = Process(target=increment_counter, args=(counter,))\n",
    "\n",
    "# Start the process\n",
    "process.start()\n",
    "\n",
    "# Wait for the process to finish\n",
    "process.join()\n",
    "\n",
    "print(\"Main process exiting.\")\n",
    "```\n",
    "\n",
    "In this example, a shared counter is created using the `Value` class. The `get_lock` method is used to acquire a lock on the counter, ensuring that only one process can modify it at a time.\n",
    "\n",
    "### 3. Parallel Processing with Pool:\n",
    "\n",
    "The `Pool` class from the `multiprocessing` module provides a convenient way to parallelize a function over multiple input values.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# Create a Pool with 3 processes\n",
    "with Pool(processes=3) as pool:\n",
    "    numbers = [1, 2, 3, 4, 5]\n",
    "    results = pool.map(square, numbers)\n",
    "\n",
    "print(\"Squared results:\", results)\n",
    "```\n",
    "\n",
    "In this example, the `square` function is applied to a list of numbers using the `map` method of the `Pool` class. The `map` method distributes the work among the specified number of processes.\n",
    "\n",
    "### 4. Process Communication with Queue:\n",
    "\n",
    "Processes can communicate with each other using the `Queue` class from the `multiprocessing` module.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def square_numbers(numbers, output):\n",
    "    for number in numbers:\n",
    "        output.put(number * number)\n",
    "\n",
    "# Create a Queue for communication\n",
    "output_queue = Queue()\n",
    "\n",
    "# Create a process that squares numbers\n",
    "numbers_to_square = [1, 2, 3, 4, 5]\n",
    "process = Process(target=square_numbers, args=(numbers_to_square, output_queue))\n",
    "\n",
    "# Start the process\n",
    "process.start()\n",
    "\n",
    "# Wait for the process to finish\n",
    "process.join()\n",
    "\n",
    "# Retrieve results from the Queue\n",
    "squared_results = []\n",
    "while not output_queue.empty():\n",
    "    squared_results.append(output_queue.get())\n",
    "\n",
    "print(\"Squared results:\", squared_results)\n",
    "```\n",
    "\n",
    "In this example, a `Queue` is used for communication between the main process and the child process. The child process squares the numbers and puts the results into the queue, which is then retrieved by the main process.\n",
    "\n",
    "### 5. Shared Resources with Manager:\n",
    "\n",
    "The `Manager` class from the `multiprocessing` module allows you to create shared objects like lists, dictionaries, and values.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "def update_shared_dict(shared_dict):\n",
    "    shared_dict['counter'] += 1\n",
    "\n",
    "# Create a Manager to manage shared resources\n",
    "with Manager() as manager:\n",
    "    shared_dict = manager.dict({'counter': 0})\n",
    "\n",
    "    # Create a process that updates the shared dictionary\n",
    "    process = Process(target=update_shared_dict, args=(shared_dict,))\n",
    "\n",
    "    # Start the process\n",
    "    process.start()\n",
    "\n",
    "    # Wait for the process to finish\n",
    "    process.join()\n",
    "\n",
    "    print(\"Updated shared dictionary:\", shared_dict)\n",
    "```\n",
    "\n",
    "In this example, a shared dictionary is created using the `Manager` class. The `update_shared_dict` function is applied to the shared dictionary in a separate process.\n",
    "\n",
    "These examples cover various aspects of multiprocessing, including creating processes, sharing data between processes, parallel processing, process communication, and using a manager to handle shared resources. Multiprocessing can be especially beneficial for CPU-bound tasks, allowing you to take advantage of multiple processor cores for parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's break down the problem-solving questions and provide an explanation for each:\n",
    "\n",
    "### 1. Parallel Matrix Multiplication:\n",
    "\n",
    "**Problem:**\n",
    "Implement a program that performs matrix multiplication using multiprocessing. Divide the task among multiple processes to achieve parallelism.\n",
    "\n",
    "**Explanation:**\n",
    "Matrix multiplication involves performing a large number of mathematical operations, making it a suitable candidate for parallelization. In this problem, you would divide the matrices into smaller blocks and assign each block multiplication to a separate process. The results are then combined to obtain the final product. This approach utilizes multiple processes to perform computations concurrently, improving performance for large matrices.\n",
    "\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "def multiply_block(a, b, result, start_row, end_row, start_col, end_col):\n",
    "    for i in range(start_row, end_row):\n",
    "        for j in range(start_col, end_col):\n",
    "            for k in range(len(b)):\n",
    "                result[i][j] += a[i][k] * b[k][j]\n",
    "\n",
    "def parallel_matrix_multiplication(a, b, result, num_processes):\n",
    "    rows_a, cols_a = len(a), len(a[0])\n",
    "    rows_b, cols_b = len(b), len(b[0])\n",
    "\n",
    "    # Check if matrix dimensions are valid for multiplication\n",
    "    if cols_a != rows_b:\n",
    "        raise ValueError(\"Invalid matrix dimensions for multiplication\")\n",
    "\n",
    "    # Split the multiplication task among processes\n",
    "    processes = []\n",
    "    rows_per_process = rows_a // num_processes\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        start_row = i * rows_per_process\n",
    "        end_row = (i + 1) * rows_per_process if i != num_processes - 1 else rows_a\n",
    "\n",
    "        process = Process(\n",
    "            target=multiply_block,\n",
    "            args=(a, b, result, start_row, end_row, 0, cols_b)\n",
    "        )\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example matrices\n",
    "    matrix_a = [\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6]\n",
    "    ]\n",
    "\n",
    "    matrix_b = [\n",
    "        [7, 8],\n",
    "        [9, 10]\n",
    "    ]\n",
    "\n",
    "    # Initialize result matrix with zeros\n",
    "    result_matrix = [[0 for _ in range(len(matrix_b[0]))] for _ in range(len(matrix_a))]\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 2\n",
    "\n",
    "    # Perform parallel matrix multiplication\n",
    "    parallel_matrix_multiplication(matrix_a, matrix_b, result_matrix, num_processes)\n",
    "\n",
    "    # Print the matrices and the result\n",
    "    print(\"Matrix A:\")\n",
    "    for row in matrix_a:\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\nMatrix B:\")\n",
    "    for row in matrix_b:\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\nResult Matrix:\")\n",
    "    for row in result_matrix:\n",
    "        print(row)\n",
    "```\n",
    "\n",
    "In this example, we have two matrices (`matrix_a` and `matrix_b`), and we want to compute their product using parallel matrix multiplication. The `multiply_block` function represents the task of multiplying a block of the result matrix. The `parallel_matrix_multiplication` function divides the multiplication task among multiple processes, and each process is responsible for computing a portion of the result matrix.\n",
    "\n",
    "You can run this script, and it will output the original matrices (`matrix_a` and `matrix_b`) and the result of their multiplication. Feel free to modify the matrices and the number of processes to experiment with different scenarios.\n",
    "\n",
    "\n",
    "\n",
    "### 2. File Processing with Pool:\n",
    "\n",
    "**Problem:**\n",
    "Write a program that processes a large text file line by line. Use a multiprocessing pool to parallelize the processing of each line and analyze the performance improvement.\n",
    "\n",
    "**Explanation:**\n",
    "For tasks like line-by-line processing of a large file, multiprocessing can be applied to concurrently process different lines. Using a multiprocessing pool, you can distribute the lines among available processes for parallel execution. This approach helps improve the overall efficiency, especially when dealing with I/O-bound tasks such as reading from a file.\n",
    "\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_line(line):\n",
    "    # Process each line (replace this with your custom processing logic)\n",
    "    return line.upper()\n",
    "\n",
    "def parallel_file_processing(file_path, num_processes):\n",
    "    # Read the lines from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Use a multiprocessing pool to parallelize line processing\n",
    "    with Pool(num_processes) as pool:\n",
    "        processed_lines = pool.map(process_line, lines)\n",
    "\n",
    "    # Write the processed lines back to the file\n",
    "    with open(file_path + \"_processed\", 'w') as output_file:\n",
    "        output_file.writelines(processed_lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example file path (replace this with your file path)\n",
    "    file_path = \"sample.txt\"\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 2\n",
    "\n",
    "    # Perform parallel file processing\n",
    "    parallel_file_processing(file_path, num_processes)\n",
    "\n",
    "    print(f\"File processing completed. Processed file saved as '{file_path}_processed'.\")\n",
    "```\n",
    "\n",
    "In this example, we have a function `process_line` that represents the processing logic for each line in the file. This function can be customized based on the specific processing you want to apply to each line.\n",
    "\n",
    "The `parallel_file_processing` function reads lines from a file, uses a multiprocessing pool to parallelize the line processing, and then writes the processed lines back to a new file. The number of processes used is determined by the `num_processes` parameter.\n",
    "\n",
    "Please replace the `file_path` variable with the path to your input file, and customize the `process_line` function according to your specific requirements. After running this script, you should see a processed file saved with the \"_processed\" suffix.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Parallel Image Processing:\n",
    "\n",
    "**Problem:**\n",
    "Create a program that applies a filter or transformation to each pixel of an image. Use multiprocessing to parallelize the pixel processing and speed up the image transformation.\n",
    "\n",
    "**Explanation:**\n",
    "Image processing often involves operations on individual pixels, making it a suitable candidate for parallelization. By dividing the image into smaller regions and assigning each region to a separate process, you can apply filters or transformations concurrently. This approach leverages the parallel processing capabilities to enhance the performance of image processing tasks.\n",
    "\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "from PIL import Image\n",
    "\n",
    "def apply_filter(pixel):\n",
    "    # Example: Apply a simple filter (replace this with your custom filter)\n",
    "    return tuple(value * 2 for value in pixel)\n",
    "\n",
    "def parallel_image_processing(image_path, output_path, num_processes):\n",
    "    # Open the image\n",
    "    with Image.open(image_path) as image:\n",
    "        # Get image dimensions\n",
    "        width, height = image.size\n",
    "\n",
    "        # Get pixel data\n",
    "        pixels = list(image.getdata())\n",
    "\n",
    "        # Use a multiprocessing pool to parallelize pixel processing\n",
    "        with Pool(num_processes) as pool:\n",
    "            processed_pixels = pool.map(apply_filter, pixels)\n",
    "\n",
    "        # Create a new image with the processed pixel data\n",
    "        processed_image = Image.new(\"RGB\", (width, height))\n",
    "        processed_image.putdata(processed_pixels)\n",
    "\n",
    "        # Save the processed image\n",
    "        processed_image.save(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example image path (replace this with your image path)\n",
    "    image_path = \"input_image.jpg\"\n",
    "\n",
    "    # Output path for the processed image\n",
    "    output_path = \"output_image.jpg\"\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 2\n",
    "\n",
    "    # Perform parallel image processing\n",
    "    parallel_image_processing(image_path, output_path, num_processes)\n",
    "\n",
    "    print(f\"Image processing completed. Processed image saved as '{output_path}'.\")\n",
    "```\n",
    "\n",
    "In this example, we are using the Python Imaging Library (PIL) module, which is commonly used for working with images. You can install it using:\n",
    "\n",
    "```bash\n",
    "pip install Pillow\n",
    "```\n",
    "\n",
    "The `apply_filter` function represents the image processing logic for each pixel. This function can be customized based on the specific filter or transformation you want to apply.\n",
    "\n",
    "The `parallel_image_processing` function opens an image, retrieves pixel data, and uses a multiprocessing pool to parallelize the pixel processing. The processed pixels are then used to create a new image, which is saved to the specified output path.\n",
    "\n",
    "Replace the `image_path` variable with the path to your input image and customize the `apply_filter` function according to your specific image processing requirements. After running this script, you should see a processed image saved at the specified output path.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Parallel Web Scraping:\n",
    "\n",
    "**Problem:**\n",
    "Build a web scraper that extracts information from multiple web pages simultaneously. Use multiprocessing to fetch and process the web pages concurrently.\n",
    "\n",
    "**Explanation:**\n",
    "Web scraping involves making requests to multiple web pages, which can be time-consuming. Using multiprocessing, you can parallelize the scraping process by assigning different URLs to separate processes. This enables fetching and processing of web pages concurrently, reducing the overall time required to collect information from multiple sources.\n",
    "\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def scrape_page(url):\n",
    "    # Example: Scrape titles of articles from a webpage (replace this with your custom scraping logic)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    titles = [title.text.strip() for title in soup.find_all('h2')]\n",
    "    return titles\n",
    "\n",
    "def parallel_web_scraping(urls, num_processes):\n",
    "    # Use a multiprocessing pool to parallelize web scraping\n",
    "    with Pool(num_processes) as pool:\n",
    "        scraped_data = pool.map(scrape_page, urls)\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example list of URLs to scrape (replace this with your URLs)\n",
    "    urls_to_scrape = [\n",
    "        'https://example.com/page1',\n",
    "        'https://example.com/page2',\n",
    "        'https://example.com/page3'\n",
    "    ]\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 2\n",
    "\n",
    "    # Perform parallel web scraping\n",
    "    scraped_results = parallel_web_scraping(urls_to_scrape, num_processes)\n",
    "\n",
    "    # Print the scraped results\n",
    "    for url, titles in zip(urls_to_scrape, scraped_results):\n",
    "        print(f\"Titles from {url}:\\n{titles}\\n\")\n",
    "```\n",
    "\n",
    "In this example, we are using the `requests` library to make HTTP requests and the `BeautifulSoup` library for HTML parsing. You can install these libraries using:\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4\n",
    "```\n",
    "\n",
    "The `scrape_page` function represents the web scraping logic for a single webpage. This function can be customized based on the specific information you want to extract from each webpage.\n",
    "\n",
    "The `parallel_web_scraping` function uses a multiprocessing pool to parallelize web scraping across multiple URLs. The list `urls_to_scrape` contains the URLs of the web pages to scrape. The result is a list of scraped data for each URL.\n",
    "\n",
    "Replace the `urls_to_scrape` variable with your list of URLs and customize the `scrape_page` function according to your specific web scraping requirements. After running this script, you should see the titles or data extracted from the specified web pages.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Parallel Search:\n",
    "\n",
    "**Problem:**\n",
    "Implement a parallel search algorithm, such as parallel binary search or parallel linear search, using multiprocessing to search for an element in a large dataset.\n",
    "\n",
    "**Explanation:**\n",
    "Searching algorithms can be parallelized by dividing the dataset into smaller chunks and assigning each chunk to a separate process. Parallel search algorithms can improve the speed of finding elements in large datasets by exploring multiple parts of the dataset concurrently.\n",
    "\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_linear_search(data, target, start, end):\n",
    "    for i in range(start, end):\n",
    "        if data[i] == target:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def parallel_search(data, target, num_processes):\n",
    "    # Split the search task among processes\n",
    "    processes = []\n",
    "    data_size = len(data)\n",
    "    chunk_size = data_size // num_processes\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = (i + 1) * chunk_size if i != num_processes - 1 else data_size\n",
    "\n",
    "        process = Pool().apply_async(parallel_linear_search, (data, target, start_index, end_index))\n",
    "        processes.append(process)\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        result = process.get()\n",
    "        if result != -1:\n",
    "            return result\n",
    "\n",
    "    return -1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example list of data (replace this with your data)\n",
    "    data_to_search = [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n",
    "\n",
    "    # Target value to search for\n",
    "    target_value = 6\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 2\n",
    "\n",
    "    # Perform parallel search\n",
    "    result_index = parallel_search(data_to_search, target_value, num_processes)\n",
    "\n",
    "    if result_index != -1:\n",
    "        print(f\"Target value {target_value} found at index {result_index}.\")\n",
    "    else:\n",
    "        print(f\"Target value {target_value} not found in the list.\")\n",
    "```\n",
    "\n",
    "In this example, we're performing a parallel linear search on a list of data using multiprocessing. The `parallel_linear_search` function represents the search logic for a specific range of the list.\n",
    "\n",
    "The `parallel_search` function divides the search task among multiple processes using the `Pool` class. Each process executes the `parallel_linear_search` function on a specific portion of the list. The result is the index where the target value is found, or -1 if the target value is not present.\n",
    "\n",
    "Replace the `data_to_search` variable with your list of data and customize the `parallel_linear_search` function or use a different search algorithm based on your specific requirements. After running this script, you should see whether the target value was found and at which index.\n",
    "\n",
    "\n",
    "\n",
    "### 6. Parallel Sorting:\n",
    "\n",
    "**Problem:**\n",
    "Design a program that performs parallel sorting of a large list using multiprocessing. Explore parallel algorithms like parallel merge sort or parallel quicksort.\n",
    "\n",
    "**Explanation:**\n",
    "Sorting algorithms, especially those based on divide and conquer, can be parallelized by dividing the dataset and sorting each portion concurrently. Parallel sorting algorithms like parallel merge sort or parallel quicksort use multiprocessing to achieve faster sorting of large datasets.\n",
    "\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def parallel_merge_sort(data):\n",
    "    if len(data) <= 1:\n",
    "        return data\n",
    "\n",
    "    mid = len(data) // 2\n",
    "    left = data[:mid]\n",
    "    right = data[mid:]\n",
    "\n",
    "    with Pool(2) as pool:\n",
    "        left = pool.apply_async(parallel_merge_sort, (left,))\n",
    "        right = pool.apply_async(parallel_merge_sort, (right,))\n",
    "        left = left.get()\n",
    "        right = right.get()\n",
    "\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    merged = []\n",
    "    left_index = 0\n",
    "    right_index = 0\n",
    "\n",
    "    while left_index < len(left) and right_index < len(right):\n",
    "        if left[left_index] < right[right_index]:\n",
    "            merged.append(left[left_index])\n",
    "            left_index += 1\n",
    "        else:\n",
    "            merged.append(right[right_index])\n",
    "            right_index += 1\n",
    "\n",
    "    merged.extend(left[left_index:])\n",
    "    merged.extend(right[right_index:])\n",
    "    return merged\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example list of data (replace this with your data)\n",
    "    data_to_sort = [random.randint(1, 100) for _ in range(10)]\n",
    "\n",
    "    print(\"Original Data:\", data_to_sort)\n",
    "\n",
    "    # Perform parallel merge sort\n",
    "    sorted_data = parallel_merge_sort(data_to_sort)\n",
    "\n",
    "    print(\"Sorted Data:\", sorted_data)\n",
    "```\n",
    "\n",
    "In this example, we're implementing a parallel merge sort using multiprocessing. The `parallel_merge_sort` function recursively divides the data into two halves and applies parallel merge sort to each half. The `merge` function then combines the sorted halves.\n",
    "\n",
    "Replace the `data_to_sort` variable with your list of data. After running this script, you should see the original data and the sorted data using parallel merge sort. Note that this is a demonstration, and for small datasets, the overhead of parallelization may outweigh the benefits. Parallel sorting is more effective for larger datasets.\n",
    "\n",
    "\n",
    "\n",
    "### 7. Distributed Task Execution:\n",
    "\n",
    "**Problem:**\n",
    "Create a system where tasks are distributed among multiple processes or machines for execution. Use multiprocessing or the `multiprocessing` module to manage the distribution and execution of tasks.\n",
    "\n",
    "**Explanation:**\n",
    "Distributed task execution involves breaking down a larger task into smaller sub-tasks and distributing them across multiple processes or machines. The `multiprocessing` module can be used for managing the distribution and coordination of tasks among different processing units.\n",
    "\n",
    "\n",
    "This problem involves creating a system where tasks are distributed among multiple processes or machines for execution. Below is a simplified example demonstrating a basic distributed computing scenario using Python's multiprocessing module and a client-server model.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "def worker_task(task_queue, result_dict):\n",
    "    while True:\n",
    "        task = task_queue.get()\n",
    "        if task is None:\n",
    "            break  # Exit when the queue is empty and all tasks are processed\n",
    "\n",
    "        # Perform the task (replace this with your custom task execution logic)\n",
    "        result = task * 2\n",
    "\n",
    "        # Store the result in the result dictionary\n",
    "        result_dict[task] = result\n",
    "\n",
    "def distribute_tasks(tasks, num_processes):\n",
    "    # Create a task queue and a result dictionary shared among processes\n",
    "    task_queue = Manager().Queue()\n",
    "    result_dict = Manager().dict()\n",
    "\n",
    "    # Add tasks to the queue\n",
    "    for task in tasks:\n",
    "        task_queue.put(task)\n",
    "\n",
    "    # Add None to signal the end of tasks\n",
    "    for _ in range(num_processes):\n",
    "        task_queue.put(None)\n",
    "\n",
    "    # Create worker processes\n",
    "    processes = [Process(target=worker_task, args=(task_queue, result_dict)) for _ in range(num_processes)]\n",
    "\n",
    "    # Start worker processes\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "\n",
    "    # Wait for worker processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example list of tasks (replace this with your tasks)\n",
    "    tasks_to_distribute = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 3\n",
    "\n",
    "    # Distribute tasks among processes\n",
    "    results = distribute_tasks(tasks_to_distribute, num_processes)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Results:\")\n",
    "    for task, result in results.items():\n",
    "        print(f\"Task: {task}, Result: {result}\")\n",
    "```\n",
    "\n",
    "In this example, the `worker_task` function represents the task execution logic. The `distribute_tasks` function distributes tasks among worker processes using a shared task queue and a result dictionary. Worker processes take tasks from the queue, perform the tasks, and store the results in the shared dictionary.\n",
    "\n",
    "Replace the `tasks_to_distribute` variable with your list of tasks, and customize the `worker_task` function with your specific task execution logic. After running this script, you should see the results of the tasks distributed and executed across multiple processes.\n",
    "\n",
    "\n",
    "\n",
    "### 8. Parallel Monte Carlo Simulation:\n",
    "\n",
    "**Problem:**\n",
    "Implement a Monte Carlo simulation that estimates a mathematical value using random sampling. Use multiprocessing to run multiple simulations concurrently, improving the accuracy of the estimate.\n",
    "\n",
    "**Explanation:**\n",
    "Monte Carlo simulations involve repeated random sampling to estimate a mathematical value. By running multiple simulations concurrently using multiprocessing, you can obtain more accurate estimates in a shorter amount of time.\n",
    "\n",
    "\n",
    "This problem involves implementing a parallel Monte Carlo simulation to estimate a mathematical value using random sampling. Here's an example using the estimation of π (pi) as a Monte Carlo simulation:\n",
    "\n",
    "```python\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def monte_carlo_simulation(total_points):\n",
    "    points_inside_circle = 0\n",
    "\n",
    "    for _ in range(total_points):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        distance = x**2 + y**2\n",
    "\n",
    "        if distance <= 1:\n",
    "            points_inside_circle += 1\n",
    "\n",
    "    return points_inside_circle\n",
    "\n",
    "def parallel_monte_carlo_simulation(total_points, num_processes):\n",
    "    points_per_process = total_points // num_processes\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.map(monte_carlo_simulation, [points_per_process] * num_processes)\n",
    "\n",
    "    total_points_inside_circle = sum(results)\n",
    "    pi_estimate = 4 * (total_points_inside_circle / total_points)\n",
    "\n",
    "    return pi_estimate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example total points for the simulation\n",
    "    total_points_to_simulate = 1000000\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 4\n",
    "\n",
    "    # Perform parallel Monte Carlo simulation\n",
    "    pi_estimate = parallel_monte_carlo_simulation(total_points_to_simulate, num_processes)\n",
    "\n",
    "    print(f\"Estimated value of π: {pi_estimate}\")\n",
    "```\n",
    "\n",
    "In this example, the `monte_carlo_simulation` function simulates random points in a unit square and estimates π by calculating the ratio of points inside a quarter circle to the total points.\n",
    "\n",
    "The `parallel_monte_carlo_simulation` function uses the `Pool` class from the `multiprocessing` module to parallelize the simulation across multiple processes. Each process runs a separate Monte Carlo simulation, and the results are combined to obtain the final estimate of π.\n",
    "\n",
    "Feel free to adjust the `total_points_to_simulate` and `num_processes` variables according to your requirements. The more points you simulate, and the more processes you use, the more accurate the estimation will be.\n",
    "\n",
    "\n",
    "\n",
    "### 9. Parallel Data Analysis:\n",
    "\n",
    "**Problem:**\n",
    "Develop a program that analyzes a large dataset, such as calculating statistical measures or aggregating data. Utilize multiprocessing to parallelize the analysis for faster results.\n",
    "\n",
    "**Explanation:**\n",
    "Data analysis tasks, such as calculating statistics or aggregating data, can benefit from parallel processing. By dividing the dataset and processing different portions concurrently, multiprocessing can significantly speed up the data analysis process.\n",
    "\n",
    "\n",
    "This problem involves developing a program that analyzes a large dataset, such as calculating statistical measures or aggregating data. Here's an example using a parallel approach with multiprocessing to speed up the data analysis:\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def analyze_data_segment(data_segment):\n",
    "    # Example: Calculate the sum of the data segment (replace this with your custom analysis logic)\n",
    "    return sum(data_segment)\n",
    "\n",
    "def parallel_data_analysis(data, num_processes):\n",
    "    data_size = len(data)\n",
    "    segment_size = data_size // num_processes\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.map(analyze_data_segment, [data[i:i+segment_size] for i in range(0, data_size, segment_size)])\n",
    "\n",
    "    total_result = sum(results)\n",
    "    return total_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset (replace this with your dataset)\n",
    "    large_dataset = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "    # Number of processes to use\n",
    "    num_processes = 2\n",
    "\n",
    "    # Perform parallel data analysis\n",
    "    result = parallel_data_analysis(large_dataset, num_processes)\n",
    "\n",
    "    print(f\"Result of parallel data analysis: {result}\")\n",
    "```\n",
    "\n",
    "In this example, the `analyze_data_segment` function represents the analysis logic for a specific segment of the dataset. The `parallel_data_analysis` function divides the dataset into segments and uses multiprocessing to parallelize the analysis. The results from each segment are then combined to obtain the total result.\n",
    "\n",
    "Replace the `large_dataset` variable with your actual dataset, and customize the `analyze_data_segment` function with your specific data analysis logic. The `num_processes` variable determines how many processes will be used for parallel analysis.\n",
    "\n",
    "This example demonstrates a simple sum calculation, but you can replace it with more complex statistical measures or any other analysis logic based on your requirements.\n",
    "\n",
    "\n",
    "\n",
    "### 10. Distributed Computing with Client-Server Model:\n",
    "\n",
    "**Problem:**\n",
    "Design a system where a server distributes tasks to multiple client processes for execution. Use multiprocessing and a client-server architecture to achieve distributed computing.\n",
    "\n",
    "**Explanation:**\n",
    "Distributed computing involves distributing tasks among multiple processing units in a network. Using multiprocessing in a client-server architecture, the server can distribute tasks to client processes, and each client can independently perform its assigned task, allowing for efficient distributed computing.\n",
    "\n",
    "\n",
    "This problem involves designing a system where a server distributes tasks to multiple client processes for execution. Here's a simple example using multiprocessing and a client-server model:\n",
    "\n",
    "**server.py**\n",
    "```python\n",
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "\n",
    "def distribute_tasks(task_queue, num_clients):\n",
    "    # Example: Distribute tasks to client processes (replace this with your custom task distribution logic)\n",
    "    tasks = [i for i in range(10)]  # Replace this with your list of tasks\n",
    "\n",
    "    # Distribute tasks to the task queue\n",
    "    for task in tasks:\n",
    "        task_queue.put(task)\n",
    "\n",
    "    # Add None to signal the end of tasks\n",
    "    for _ in range(num_clients):\n",
    "        task_queue.put(None)\n",
    "\n",
    "def server(task_queue, result_queue):\n",
    "    # Number of client processes\n",
    "    num_clients = 3\n",
    "\n",
    "    # Create and start client processes\n",
    "    client_processes = [Process(target=client, args=(task_queue, result_queue)) for _ in range(num_clients)]\n",
    "    for process in client_processes:\n",
    "        process.start()\n",
    "\n",
    "    # Distribute tasks to client processes\n",
    "    distribute_tasks(task_queue, num_clients)\n",
    "\n",
    "    # Wait for client processes to finish\n",
    "    for process in client_processes:\n",
    "        process.join()\n",
    "\n",
    "def client(task_queue, result_queue):\n",
    "    while True:\n",
    "        task = task_queue.get()\n",
    "        if task is None:\n",
    "            break  # Exit when there are no more tasks\n",
    "\n",
    "        # Example: Perform the task (replace this with your custom task execution logic)\n",
    "        result = task * 2\n",
    "\n",
    "        # Store the result in the result queue\n",
    "        result_queue.put(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    task_queue = Queue()\n",
    "    result_queue = Queue()\n",
    "\n",
    "    # Start the server process\n",
    "    server_process = Process(target=server, args=(task_queue, result_queue))\n",
    "    server_process.start()\n",
    "\n",
    "    # Wait for the server process to finish\n",
    "    server_process.join()\n",
    "\n",
    "    # Retrieve and print results from the result queue\n",
    "    print(\"Results:\")\n",
    "    while not result_queue.empty():\n",
    "        result = result_queue.get()\n",
    "        print(result)\n",
    "```\n",
    "\n",
    "In this example, the server distributes tasks to client processes through a task queue. The `distribute_tasks` function in the server distributes a list of tasks to the task queue, and each client process executes tasks from the queue. The `None` sentinel value is used to signal the end of tasks.\n",
    "\n",
    "**Note:** This is a simplified example, and in a real-world scenario, you may need to handle more complex tasks, communication between processes, and error handling. Adjust the code based on your specific requirements.\n",
    "\n",
    "To run this example, save it in two separate files (e.g., `server.py` and `client.py`) and execute them separately. The server will distribute tasks to client processes, and the clients will execute the tasks and store the results in a result queue. The main program then retrieves and prints the results from the result queue.\n",
    "\n",
    "\n",
    "\n",
    "### 11. Parallel Genetic Algorithm:\n",
    "\n",
    "**Problem:**\n",
    "Implement a parallel version of a genetic algorithm to optimize a solution space. Use multiprocessing to evaluate multiple candidate solutions concurrently.\n",
    "\n",
    "**Explanation:**\n",
    "Genetic algorithms involve evaluating multiple candidate solutions in parallel. By using multiprocessing, the fitness evaluation of different candidate solutions can be performed concurrently, accelerating the optimization process.\n",
    "\n",
    "\n",
    "This problem involves implementing a parallel version of a genetic algorithm to optimize a solution space. Here's a basic example using multiprocessing to evaluate multiple candidate solutions concurrently:\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def evaluate_fitness(candidate):\n",
    "    # Example: Evaluate the fitness of a candidate solution (replace this with your custom fitness function)\n",
    "    return sum(candidate)\n",
    "\n",
    "def generate_random_candidate():\n",
    "    # Example: Generate a random candidate solution (replace this with your custom generation logic)\n",
    "    return [random.randint(0, 10) for _ in range(5)]\n",
    "\n",
    "def parallel_genetic_algorithm(population_size, num_generations, num_processes):\n",
    "    # Generate an initial population of random candidates\n",
    "    population = [generate_random_candidate() for _ in range(population_size)]\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        # Evaluate the fitness of each candidate in parallel\n",
    "        with Pool(num_processes) as pool:\n",
    "            fitness_scores = pool.map(evaluate_fitness, population)\n",
    "\n",
    "        # Select the top candidates based on fitness scores (replace this with your custom selection logic)\n",
    "        selected_indices = sorted(range(len(fitness_scores)), key=lambda k: fitness_scores[k], reverse=True)[:population_size]\n",
    "\n",
    "        # Create the next generation by crossover and mutation (replace this with your custom genetic operations)\n",
    "        next_generation = []\n",
    "        for _ in range(population_size):\n",
    "            parent1 = population[random.choice(selected_indices)]\n",
    "            parent2 = population[random.choice(selected_indices)]\n",
    "            crossover_point = random.randint(0, len(parent1))\n",
    "            child = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "            mutation_rate = 0.1\n",
    "            if random.random() < mutation_rate:\n",
    "                mutation_point = random.randint(0, len(child) - 1)\n",
    "                child[mutation_point] = random.randint(0, 10)\n",
    "            next_generation.append(child)\n",
    "\n",
    "        # Replace the old population with the new generation\n",
    "        population = next_generation\n",
    "\n",
    "    # Return the best candidate after the specified number of generations\n",
    "    best_candidate = population[fitness_scores.index(max(fitness_scores))]\n",
    "    return best_candidate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters\n",
    "    population_size = 10\n",
    "    num_generations = 5\n",
    "    num_processes = 2\n",
    "\n",
    "    # Run the parallel genetic algorithm\n",
    "    best_solution = parallel_genetic_algorithm(population_size, num_generations, num_processes)\n",
    "\n",
    "    print(\"Best Solution:\", best_solution)\n",
    "```\n",
    "\n",
    "In this example, the `evaluate_fitness` function represents the fitness evaluation for a candidate solution. The `generate_random_candidate` function creates a random candidate solution. The `parallel_genetic_algorithm` function runs the genetic algorithm in parallel, evaluating fitness scores using multiprocessing and evolving the population through crossover and mutation.\n",
    "\n",
    "Replace the fitness evaluation, candidate generation, selection, crossover, and mutation functions with your specific requirements for the optimization problem you are addressing. This is a basic template, and the effectiveness of the genetic algorithm depends on the nature of your optimization problem.\n",
    "\n",
    "\n",
    "\n",
    "### 12. Parallel Word Count:\n",
    "\n",
    "**Problem:**\n",
    "Create a program that counts the occurrences of words in a large text corpus. Use multiprocessing to process different segments of the text concurrently and aggregate the results.\n",
    "\n",
    "**Explanation:**\n",
    "Word counting in a large text corpus can be parallelized by dividing the text into segments and processing each segment concurrently. Using multiprocessing, word occurrences can be counted in parallel\n",
    "\n",
    "This problem involves creating a distributed system for collaborative editing, where multiple users can simultaneously edit a shared document. Here's a simplified example using Python's `socket` module for communication between a server and clients:\n",
    "\n",
    "**server.py:**\n",
    "```python\n",
    "import socket\n",
    "import threading\n",
    "\n",
    "class CollaborativeEditorServer:\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.clients = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.document = \"\"\n",
    "\n",
    "    def start(self):\n",
    "        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        server_socket.bind((self.host, self.port))\n",
    "        server_socket.listen()\n",
    "\n",
    "        print(f\"Server listening on {self.host}:{self.port}\")\n",
    "\n",
    "        while True:\n",
    "            client_socket, client_address = server_socket.accept()\n",
    "            print(f\"Accepted connection from {client_address}\")\n",
    "            client_thread = threading.Thread(target=self.handle_client, args=(client_socket,))\n",
    "            client_thread.start()\n",
    "\n",
    "    def broadcast(self, message):\n",
    "        with self.lock:\n",
    "            for client in self.clients:\n",
    "                try:\n",
    "                    client.send(message.encode())\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    def handle_client(self, client_socket):\n",
    "        with self.lock:\n",
    "            self.clients.append(client_socket)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                data = client_socket.recv(1024).decode()\n",
    "                if not data:\n",
    "                    break\n",
    "                print(f\"Received from client: {data}\")\n",
    "\n",
    "                # Handle collaborative editing logic here\n",
    "                # For simplicity, just append the received data to the document\n",
    "                with self.lock:\n",
    "                    self.document += data\n",
    "\n",
    "                # Broadcast the updated document to all clients\n",
    "                self.broadcast(self.document)\n",
    "\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        with self.lock:\n",
    "            self.clients.remove(client_socket)\n",
    "\n",
    "        client_socket.close()\n",
    "        print(\"Connection closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server = CollaborativeEditorServer(\"127.0.0.1\", 5000)\n",
    "    server.start()\n",
    "```\n",
    "\n",
    "**client.py:**\n",
    "```python\n",
    "import socket\n",
    "import threading\n",
    "\n",
    "class CollaborativeEditorClient:\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.username = input(\"Enter your username: \")\n",
    "\n",
    "    def start(self):\n",
    "        self.client_socket.connect((self.host, self.port))\n",
    "\n",
    "        receive_thread = threading.Thread(target=self.receive_messages)\n",
    "        receive_thread.start()\n",
    "\n",
    "        while True:\n",
    "            message = input()\n",
    "            if message.lower() == \"/exit\":\n",
    "                break\n",
    "            self.client_socket.send(f\"{self.username}: {message}\".encode())\n",
    "\n",
    "        self.client_socket.close()\n",
    "\n",
    "    def receive_messages(self):\n",
    "        while True:\n",
    "            try:\n",
    "                data = self.client_socket.recv(1024).decode()\n",
    "                if not data:\n",
    "                    break\n",
    "                print(data)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = CollaborativeEditorClient(\"127.0.0.1\", 5000)\n",
    "    client.start()\n",
    "```\n",
    "\n",
    "In this example, the `CollaborativeEditorServer` class handles the server logic, including accepting connections, handling clients in separate threads, and broadcasting changes to all connected clients. The `CollaborativeEditorClient` class represents a simple client that connects to the server, sends messages, and receives updates from other clients.\n",
    "\n",
    "To run this example, execute the `server.py` script in one terminal window and run the `client.py` script in multiple terminal windows for each client. The clients can simultaneously edit the shared document, and their changes will be broadcasted to all other connected clients.\n",
    "\n",
    "Please note that this example is minimalistic, and a production-level collaborative editing system would require more sophisticated handling of document changes, user interactions, error handling, and security considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my main program..\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def test():\n",
    "    print(\"This is my Multiprocessing Program.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = multiprocessing.Process(target=test) \n",
    "    print(\"This is my main program..\")\n",
    "    m.start()\n",
    "    m.join()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my Multiprocessing Program.\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ali\n"
     ]
    }
   ],
   "source": [
    "print(\"ali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def square(n):\n",
    "    return n**2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes=5) as pool:\n",
    "        outcome = pool.map(square, [1,25,3,5,3,5,3,74,45,2,4,2])\n",
    "        print(outcome)   # [1, 625, 9, 25, 9, 25, 9, 5476, 2025, 4, 16, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "\n",
    "def producer(q):\n",
    "    for i in ['ali', 'abbas', 'jawed', 'paikar', 'haider']:\n",
    "        q.put(i)\n",
    "\n",
    "def consumer(q):\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        print(item)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    queue = multiprocessing.Queue()\n",
    "    m1 = multiprocessing.Process(target=producer, args=(queue,))\n",
    "    m2 = multiprocessing.Process(target=consumer, args=(queue,))\n",
    "\n",
    "    m1.start()\n",
    "    m2.start()\n",
    "    queue.put(\"xyz\")\n",
    "    m1.join()\n",
    "    m2.join()       \n",
    "\n",
    "# Output\n",
    "# xyz\n",
    "# ali\n",
    "# abbas\n",
    "# jawed\n",
    "# paikar\n",
    "# haider             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def square(index, value):\n",
    "    value[index] = value[index] ** 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arr = multiprocessing.Array('i', [2,3,6,7,8,8,9,3,3,3])\n",
    "    process = []\n",
    "    for i in range(10):\n",
    "        m = multiprocessing.Process(target=square, args=(i, arr))\n",
    "        process.append(m)\n",
    "        m.start()\n",
    "    for m in process:\n",
    "        m.join()\n",
    "\n",
    "    print(list(arr))         # [4, 9, 36, 49, 64, 64, 81, 9, 9, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def sender(conn, msg):\n",
    "    for i in msg:\n",
    "        conn.send(i)\n",
    "    conn.close()\n",
    "\n",
    "def receiver(conn):\n",
    "    while True:\n",
    "        try:\n",
    "            msg = conn.recv()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        print(msg) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    msg = [\"My name is Ali Abbas.\", \"This is my messasge.\", \"My nickname is Jawed..\", \"I live in Delhi.\"]\n",
    "    parent_conn, child_conn = multiprocessing.Pipe()\n",
    "    p1 = multiprocessing.Process(target=sender, args=(child_conn, msg))\n",
    "    p2 = multiprocessing.Process(target=receiver, args=(parent_conn,))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p1.join()\n",
    "    child_conn.close()\n",
    "    p2.join()\n",
    "    parent_conn.close()               \n",
    "\n",
    "# Output\n",
    "# My name is Ali Abbas.\n",
    "# This is my messasge.\n",
    "# My nickname is Jawed..\n",
    "# I live in Delhi.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
